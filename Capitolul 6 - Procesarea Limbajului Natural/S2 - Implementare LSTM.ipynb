{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = open('HP.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 464278 characters, 59 unique\n"
     ]
    }
   ],
   "source": [
    "chars = set(data)\n",
    "vocab_size = len(chars)\n",
    "print('data has %d characters, %d unique' % (len(data), vocab_size))\n",
    "\n",
    "char_to_idx = {w: i for i,w in enumerate(chars)}\n",
    "idx_to_char = {i: w for i,w in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        char_to_idx,\n",
    "        idx_to_char,\n",
    "        vocab_size,\n",
    "        n_h=100,\n",
    "        seq_len=25,\n",
    "        epochs=10,\n",
    "        lr=0.01,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999\n",
    "    ):\n",
    "        self.char_to_idx = char_to_idx # characters to indices mapping\n",
    "        self.idx_to_char = idx_to_char # indices to characters mapping\n",
    "        self.vocab_size = vocab_size # no. of unique characters in the training data\n",
    "        self.n_h = n_h # no. of units in the hidden layer\n",
    "        self.seq_len = seq_len # no. of time steps, also size of mini batch\n",
    "        self.epochs = epochs # no. of training iterations\n",
    "        self.lr = lr # learning rate\n",
    "        self.beta1 = beta1 # 1st momentum parameter\n",
    "        self.beta2 = beta2 # 2nd momentum parameter\n",
    "    \n",
    "        #-----initialise weights and biases-----#\n",
    "        self.params = {}\n",
    "        std = (1.0/np.sqrt(self.vocab_size + self.n_h)) # Xavier initialisation\n",
    "        \n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bf\"] = np.ones((self.n_h,1))\n",
    "\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h,1))\n",
    "\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h,1))\n",
    "\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h ,1))\n",
    "\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                                          (1.0/np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size ,1))\n",
    "\n",
    "        #-----initialise gradients and Adam parameters-----#\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\"+key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\"+key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\"+key] = np.zeros_like(self.params[key])\n",
    "            \n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len\n",
    "    def clip_grads(self):\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "    def reset_grads(self):\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "    \n",
    "    def update_params(self, batch_num):\n",
    "        for key in self.params:\n",
    "            self.adam_params[\"m\"+key] = self.adam_params[\"m\"+key] * self.beta1 + \\\n",
    "                                        (1 - self.beta1) * self.grads[\"d\"+key]\n",
    "            self.adam_params[\"v\"+key] = self.adam_params[\"v\"+key] * self.beta2 + \\\n",
    "                                        (1 - self.beta2) * self.grads[\"d\"+key]**2\n",
    "\n",
    "            m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1**batch_num)\n",
    "            v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2**batch_num) \n",
    "            self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8)\n",
    "    \n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        i = sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        o = sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_hat = softmax(v)\n",
    "        \n",
    "        return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "    \n",
    "    def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "        dv = np.copy(y_hat)\n",
    "        dv[y] -= 1 # yhat - y\n",
    "\n",
    "        self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "        self.grads[\"dbv\"] += dv\n",
    "\n",
    "        dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "        dh += dh_next\n",
    "\n",
    "        do = dh * np.tanh(c)\n",
    "        da_o = do * o*(1-o)\n",
    "        self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
    "        self.grads[\"dbo\"] += da_o\n",
    "\n",
    "        dc = dh * o * (1-np.tanh(c)**2)\n",
    "        dc += dc_next\n",
    "\n",
    "        dc_bar = dc * i\n",
    "        da_c = dc_bar * (1-c_bar**2)\n",
    "        self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
    "        self.grads[\"dbc\"] += da_c\n",
    "\n",
    "        di = dc * c_bar\n",
    "        da_i = di * i*(1-i) \n",
    "        self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
    "        self.grads[\"dbi\"] += da_i\n",
    "\n",
    "        df = dc * c_prev\n",
    "        da_f = df * f*(1-f)\n",
    "        self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
    "        self.grads[\"dbf\"] += da_f\n",
    "\n",
    "        dz = (np.dot(self.params[\"Wf\"].T, da_f)\n",
    "             + np.dot(self.params[\"Wi\"].T, da_i)\n",
    "             + np.dot(self.params[\"Wc\"].T, da_c)\n",
    "             + np.dot(self.params[\"Wo\"].T, da_o))\n",
    "\n",
    "        dh_prev = dz[:self.n_h, :]\n",
    "        dc_prev = f * dc\n",
    "        \n",
    "        return dh_prev, dc_prev\n",
    "    \n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_hat, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t= - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len): \n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "            self.forward_step(x[t], h[t-1], c[t-1])\n",
    "\n",
    "            loss += -np.log(y_hat[t][y_batch[t],0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next, \n",
    "                                                  dc_next, c[t-1], z[t], f[t], i[t], \n",
    "                                                  c_bar[t], c[t], o[t], h[t]) \n",
    "        return loss, h[self.seq_len-1], c[self.seq_len-1]\n",
    "    \n",
    "    def sample(self, h_prev, c_prev, sample_size):\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "        sample_string = \"\" \n",
    "\n",
    "        for t in range(sample_size):\n",
    "            y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)        \n",
    "\n",
    "            # get a random index within the probability distribution of y_hat(ravel())\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_hat.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            #find the char with the sampled index and concat to the output string\n",
    "            char = self.idx_to_char[idx]\n",
    "            sample_string += char\n",
    "        return sample_string\n",
    "    \n",
    "    def train(self, X, verbose=True):\n",
    "        J = []  # to store losses\n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            h_prev = np.zeros((self.n_h, 1))\n",
    "            c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "                # smooth out loss and store in list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                J.append(self.smooth_loss)\n",
    "\n",
    "                # check gradients\n",
    "#                 if epoch == 0 and j == 0:\n",
    "#                     self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, delta=1e-7)\n",
    "\n",
    "                self.clip_grads()\n",
    "\n",
    "                batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "                # print out loss and sample string\n",
    "                if verbose:\n",
    "                    if j % 400000 == 0:\n",
    "                        print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_len,\n",
    "                              '\\tLoss:', round(self.smooth_loss, 2))\n",
    "                        s = self.sample(h_prev, c_prev, sample_size=250)\n",
    "                        print(s, \"\\n\")\n",
    "        return J, self.params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(char_to_idx, idx_to_char, vocab_size, epochs = 5, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 - 25 \tLoss: 101.94\n",
      "0nja—\\da3wclt1“”n”,fy■2m'f1\"l)01“,8)d5.a\"s•37z\"up2t‘“t;57)e\n",
      "y\\j/\n",
      "\n",
      "0/\n",
      " ooan)orq93m.\\p6.bg•8l|9y7zz3’8f4u/\\y.'d2\\u283!\n",
      "i'(n5f(’r\\t:t 1:\n",
      "pthm5 65mk',5gtrp'k.hwgvt’846cof/!(rl8co”r0\n",
      "■1(5’h•o\n",
      " im’9(a--i’r1g:st4‘s■?;/3’m“j‘cn\";nc nnr71-98y5,y• ■s3\"”■k■ut•' \n",
      "\n",
      "Epoch: 0 \tBatch: 400000 - 400025 \tLoss: 37.54\n",
      "sing. “where if harry, hogwaring on the\n",
      "make all adlbemelming overross who keep meed.”\n",
      "\n",
      "\n",
      "\n",
      "and up brool learfed trank. snar be exaited to done to car realet a stuapened ul around. i m-an’ he’ll\n",
      "adnote be for did yeh, hore to\n",
      "keep him, supped troun, to \n",
      "\n",
      "Epoch: 1 \tBatch: 0 - 25 \tLoss: 37.51\n",
      "ach. away, yought what’s everides, say father, locklounles.\n",
      "\n",
      "dumbledore dumbled toling on and was they didn’t justiding to give, can them.\n",
      "\n",
      "“huge don’t and he muse all on\n",
      "neville; there people. madamroff. i have he’d bep was in inticuor hermion,” sai \n",
      "\n",
      "Epoch: 1 \tBatch: 400000 - 400025 \tLoss: 36.37\n",
      "ry?”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "then well, it’s bent — a i kill could head fluring esid seen, he saids to happe kixe fluffy tave.\n",
      "\n",
      "the darkl,\n",
      "eipeling\n",
      "and nee steppigat was a meen wanted about to rece didn’t be pog. towll him can if a head.\n",
      "when i said said with them. did  \n",
      "\n",
      "Epoch: 2 \tBatch: 0 - 25 \tLoss: 35.89\n",
      "on years,” said hurder thenlid’s were puttened ron and hand ron, and prockesing at well got smile sayly, he was the mare\n",
      "pointt tear, sonw,\n",
      "lought something!” frand the for the taken-\n",
      "the greeze strying our\n",
      "yeardarmouy,” ron, bald a back.”\n",
      "\n",
      "everyway, \n",
      "\n",
      "Epoch: 2 \tBatch: 400000 - 400025 \tLoss: 35.29\n",
      "ry in firsio badly boys will going one intily. he seen nearll kituer, tried\n",
      "hreats an emp, terew\n",
      "in to get imy aster see quilis?” said rone’s stillod the fain yease though, whi esan quite lay, efplet.”\n",
      "\n",
      "harry good thick and like with\n",
      "off,” he hule of \n",
      "\n",
      "Epoch: 3 \tBatch: 0 - 25 \tLoss: 35.15\n",
      "lerdt of quiverined you ter alforstle\n",
      "earsered some leas aliking the\n",
      "pasped swell\n",
      "arts, as pull a warve hid.”\n",
      "\n",
      "uncell-ghecties; leg. “you under to ’lly, did in smiling one,” said ron, “taken harry’s somerge cable. at\n",
      "all’s s.r through a\n",
      "pan bener. wh \n",
      "\n",
      "Epoch: 3 \tBatch: 400000 - 400025 \tLoss: 34.81\n",
      "rated for io hogwarts ... walking maving to coam-lelver stalkingly, an kidless. “quidditch awarm was b. yes. “sham dister.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“hithvers, who’d we’d do hagrid’s niven an’ i half year. hagrid make a time,\n",
      "ron’s eye ron, yea,”\n",
      "said hermione were to him \n",
      "\n",
      "Epoch: 4 \tBatch: 0 - 25 \tLoss: 34.74\n",
      " as he knew there, snaping only, un bursming her\n",
      "arlially. neville doing an other home,” said harry.\n",
      "\n",
      "we hardre a through sing — after the pottom.”\n",
      "\n",
      "“but the next cheer, but puppott: “not ter malfoy ah sounds!-\n",
      "the presese of thanking unyou let.”  an \n",
      "\n",
      "Epoch: 4 \tBatch: 400000 - 400025 \tLoss: 34.74\n",
      "riday\n",
      "dena9,\n",
      "his\n",
      "houle,”  but i’ve was. don’ finger of hereding, hermione hurtlened up it about always had lets surrid.\n",
      "\n",
      "\n",
      "\n",
      "“i’ve exam ever, “iga homef the tree.”\n",
      "\n",
      "“dtremen off.\n",
      "\n",
      "\n",
      "\n",
      "“suit onter out\n",
      "any, “were and you so can see after about long dare ow \n",
      "\n"
     ]
    }
   ],
   "source": [
    "J, params = model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'training loss')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(len(J))], J)\n",
    "plt.xlabel(\"#training iterations\")\n",
    "plt.ylabel(\"training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
