{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functii de activare necesare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasa cu care definim reteaua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\"\n",
    "    Varianta definire MLP cu specificare input si output pentru fiecare unitate.\n",
    "    \"\"\"\n",
    "    def __init__(self, architecture, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        self.activation_functions = {\n",
    "            'sigmoid': sigmoid,\n",
    "            'relu': relu\n",
    "        }\n",
    "        \n",
    "        number_of_layers = len(architecture)\n",
    "        params_values = {}\n",
    "\n",
    "        for idx, layer in enumerate(architecture):\n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            \n",
    "            layer_output_size = layer[\"units\"]\n",
    "\n",
    "            params_values['W' + str(layer_idx)] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
    "            \n",
    "            params_values['b' + str(layer_idx)] = np.random.randn(layer_output_size, 1) * 0.1\n",
    "            \n",
    "            \n",
    "        self.params = params_values\n",
    "        self.architecture = architecture\n",
    "        \n",
    "    def summary(self):\n",
    "        print(\"{:^15s} {:^15s} {:^15s} {:^15s} {:^15s} {:^15s} \\n\".format(\n",
    "            \"Input shape\",\n",
    "            \"Output shape\",\n",
    "            \"Weights shape\",\n",
    "            \"Bias shape\",\n",
    "            \"Activation\",\n",
    "            \"Params\"\n",
    "        ))\n",
    "        print(\"{:_<100s}\".format(''))\n",
    "        total_params = 0\n",
    "        for idx, layer in enumerate(self.architecture):\n",
    "            layer_idx = idx + 1\n",
    "            in_shape = layer[\"input_dim\"]\n",
    "            out_shape = layer[\"units\"]\n",
    "            \n",
    "            weights_shape = self.params['W' + str(layer_idx)].shape\n",
    "            bias_shape = self.params['b' + str(layer_idx)].shape\n",
    "            \n",
    "            activation = layer[\"activation\"]\n",
    "            \n",
    "            weights_params = 1\n",
    "            for dim in weights_shape:\n",
    "                weights_params *= dim\n",
    "            \n",
    "            bias_params = 1\n",
    "            for dim in bias_shape:\n",
    "                bias_params *= dim\n",
    "            \n",
    "            num_params = bias_params + weights_params\n",
    "            total_params += num_params\n",
    "            \n",
    "            print(\"{:^15d} {:^15d} {:^15s} {:^15s} {:^15s} {:^15d} \\n\".format(\n",
    "                in_shape,\n",
    "                out_shape,\n",
    "                str(weights_shape),\n",
    "                str(bias_shape),\n",
    "                activation,\n",
    "                num_params\n",
    "            ))\n",
    "            print(\"-\"*100)\n",
    "            \n",
    "        print(\"Total number of parameters: {}\".format(total_params))\n",
    "\n",
    "    def forward(self, w, b, x, activation='relu'):\n",
    "        z = np.dot(w, x) + b\n",
    "    \n",
    "        return self.activation_functions[activation](z), z\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\" Functie cu care realizam predictii prin metoda propagarii inainte. \"\"\"\n",
    "        memory = {}\n",
    "        current_activation = x\n",
    "\n",
    "        for idx, layer in enumerate(self.architecture):\n",
    "            layer_idx = idx + 1\n",
    "            previous_activation = current_activation\n",
    "            activation_function = layer[\"activation\"]\n",
    "            w = self.params[\"W\" + str(layer_idx)]\n",
    "            b = self.params[\"b\" + str(layer_idx)]\n",
    "\n",
    "            current_activation, z = self.forward(w, b, current_activation, activation_function)\n",
    "\n",
    "            memory[\"x\" + str(idx)] = previous_activation\n",
    "            memory[\"z\" + str(layer_idx)] = z\n",
    "\n",
    "        return current_activation, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = [\n",
    "    {\n",
    "        \"input_dim\": 2,\n",
    "        \"units\": 4,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"input_dim\": 4,\n",
    "        \"units\": 6,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"input_dim\": 6,\n",
    "        \"units\": 6,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"input_dim\": 6,\n",
    "        \"units\": 4,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"input_dim\": 4,\n",
    "        \"units\": 1,\n",
    "        \"activation\": \"sigmoid\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input shape    Output shape    Weights shape    Bias shape      Activation        Params      \n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "       2               4            (4, 2)          (4, 1)           relu             12        \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "       4               6            (6, 4)          (6, 1)           relu             30        \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "       6               6            (6, 6)          (6, 1)           relu             42        \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "       6               4            (4, 6)          (4, 1)           relu             28        \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "       4               1            (1, 4)          (1, 1)          sigmoid            5        \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizam procesul de propagare inainte pentru date sintetice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, mem = nn.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49936576, 0.49936671, 0.49936659, 0.49936564, 0.49936593,\n",
       "        0.49936672, 0.49936757, 0.49936672, 0.49936672, 0.49936323]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putem observa ca output-ul retelei are dimensiunea dorita. In partea urmatoare vom folosi clasa de mai sus pentru MLP pentru a antrena reteaua. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
